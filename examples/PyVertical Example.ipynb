{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Simple Vertically Partitioned Split Neural Network\n",
    "\n",
    "- <b>Alice</b>\n",
    "    - Has model Segment 1\n",
    "    - Has the handwritten Images\n",
    "- <b>Bob</b>\n",
    "    - Has model Segment 2\n",
    "    - Has the image Labels\n",
    "    \n",
    "Based on [SplitNN - Tutorial 3](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/split_neural_network/Tutorial%203%20-%20Folded%20Split%20Neural%20Network.ipynb) from Adam J Hall - Twitter: [@AJH4LL](https://twitter.com/AJH4LL) 路 GitHub:  [@H4LL](https://github.com/H4LL)\n",
    "\n",
    "Authors:\n",
    "- Pavlos Papadopoulos 路 GitHub:  [@pavlos-p](https://github.com/pavlos-p)\n",
    "- Tom Titcombe 路 GitHub:  [@TTitcombe](https://github.com/TTitcombe)\n",
    "- Robert Sandmann 路 GitHub: [@rsandmann](https://github.com/rsandmann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "from src.dataloader import VerticalDataLoader\n",
    "from src.psi.util import Client, Server\n",
    "from src.splitnn import SplitNN\n",
    "from src.utils import add_ids\n",
    "from src.distribute_data import Distribute_MNIST\n",
    "\n",
    "hook = sy.TorchHook(torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./VerticalDataset/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./VerticalDataset/raw/train-images-idx3-ubyte.gz to ./VerticalDataset/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./VerticalDataset/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./VerticalDataset/raw/train-labels-idx1-ubyte.gz to ./VerticalDataset/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./VerticalDataset/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./VerticalDataset/raw/t10k-images-idx3-ubyte.gz to ./VerticalDataset/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./VerticalDataset/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./VerticalDataset/raw/t10k-labels-idx1-ubyte.gz to ./VerticalDataset/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# create some workers\n",
    "client_1 = sy.VirtualWorker(hook, id=\"client_1\")\n",
    "client_2 = sy.VirtualWorker(hook, id=\"client_2\")\n",
    "server = sy.VirtualWorker(hook, id= \"server\") \n",
    "\n",
    "data_owners = (client_1, client_2)\n",
    "model_locations = [client_1, client_2, server]\n",
    "\n",
    "#Split each image and send one part to client_1, and other to client_2\n",
    "distributed_trainloader = Distribute_MNIST(data_owners=data_owners, data_loader=trainloader)"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Define our model segments\n",
    "\n",
    "input_size= [28*14, 28*14]\n",
    "hidden_sizes= {\"client_1\": [32, 64], \"client_2\":[32, 64], \"server\":[128, 64]}\n",
    "output_size = 10\n",
    "\n",
    "models = {\n",
    "    \"client_1\": nn.Sequential(\n",
    "                nn.Linear(input_size[0], hidden_sizes[\"client_1\"][0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_sizes[\"client_1\"][0], hidden_sizes[\"client_1\"][1]),\n",
    "                nn.ReLU(),\n",
    "    ),\n",
    "    \"client_2\":  nn.Sequential(\n",
    "                nn.Linear(input_size[1], hidden_sizes[\"client_2\"][0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_sizes[\"client_2\"][0], hidden_sizes[\"client_2\"][1]),\n",
    "                nn.ReLU(),\n",
    "    ),\n",
    "    \"server\": nn.Sequential(\n",
    "                nn.Linear(hidden_sizes[\"server\"][0], hidden_sizes[\"server\"][1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_sizes[\"server\"][1], 10),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [\n",
    "    optim.SGD(models[location.id].parameters(), lr=0.05,)\n",
    "    for location in model_locations\n",
    "]\n",
    "\n",
    "for location in model_locations:\n",
    "    models[location.id].send(location)\n",
    "\n",
    "\n",
    "#Instantiate a SpliNN class with our distributed segments and their respective optimizers\n",
    "splitNN = SplitNN(models, server, data_owners, optimizers)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Epoch 0 - Training loss: 1.141 - Accuracy: 73.202\n",
      "Epoch 1 - Training loss: 0.384 - Accuracy: 89.162\n",
      "Epoch 2 - Training loss: 0.317 - Accuracy: 90.818\n",
      "Epoch 3 - Training loss: 0.281 - Accuracy: 91.948\n",
      "Epoch 4 - Training loss: 0.254 - Accuracy: 92.738\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    #iterate over each datapoint \n",
    "    for data_ptr, label in distributed_trainloader:\n",
    "        \n",
    "        #send labels to server's location for training\n",
    "        label = label.send(server)\n",
    "        \n",
    "        loss = splitNN.train(data_ptr, label)\n",
    "        running_loss += loss\n",
    "\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(trainloader)))"
   ]

  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
